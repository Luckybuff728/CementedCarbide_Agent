# è¿­ä»£ä¼˜åŒ–åŠŸèƒ½ - æŠ€æœ¯æ–¹æ¡ˆè®¾è®¡ï¼ˆç¬¬2éƒ¨åˆ†ï¼‰

## ğŸ“‹ æ–¹æ¡ˆæ¦‚è§ˆ

åŸºäºç¬¬1éƒ¨åˆ†çš„æ¶æ„åˆ†æï¼Œæœ¬æ–‡æ¡£æä¾›è¯¦ç»†çš„æŠ€æœ¯è®¾è®¡æ–¹æ¡ˆã€‚

---

## 1ï¸âƒ£ åç«¯å·¥ä½œæµé‡æ„

### 1.1 æ–°å¢èŠ‚ç‚¹

**éœ€è¦æ·»åŠ çš„èŠ‚ç‚¹ï¼š**

```python
# src/graph/nodes.py

def experiment_workorder_generation_node(state: CoatingWorkflowState) -> Dict:
    """ç”Ÿæˆå®éªŒå·¥å•èŠ‚ç‚¹ - é›†æˆç°æœ‰workorder_service"""
    # âš ï¸ ä½¿ç”¨å·²å­˜åœ¨çš„workorder_serviceï¼Œä¸éœ€è¦åˆ›å»ºæ–°çš„
    from ..services.workorder_service import generate_workorder
    
    logger.info(f"[å·¥å•ç”Ÿæˆ] ä»»åŠ¡ {state['task_id']}, è¿­ä»£ {state['current_iteration']}")
    
    # ç¡®å®šç”¨æˆ·é€‰æ‹©çš„æ–¹æ¡ˆ
    selected_option = state.get("selected_optimization_type")  # "P1", "P2", "P3"
    if not selected_option:
        # å¦‚æœç”¨æˆ·æœªé€‰æ‹©ï¼Œé»˜è®¤ä½¿ç”¨P1
        logger.warning("ç”¨æˆ·æœªé€‰æ‹©æ–¹æ¡ˆï¼Œé»˜è®¤ä½¿ç”¨P1")
        selected_option = "P1"
    
    # æµå¼è¾“å‡ºå›è°ƒ
    def stream_callback(node: str, content: str):
        send_stream_chunk_sync("experiment_workorder", content)
    
    # è°ƒç”¨ç°æœ‰çš„å·¥å•ç”ŸæˆæœåŠ¡
    result = generate_workorder(
        task_id=state['task_id'],
        selected_option=selected_option,
        task_state=state,
        stream_callback=stream_callback
    )
    
    if not result.get("success"):
        logger.error(f"å·¥å•ç”Ÿæˆå¤±è´¥: {result.get('error')}")
        return {
            "error_message": result.get("error"),
            "workflow_status": "error"
        }
    
    logger.info(f"[å·¥å•ç”Ÿæˆ] å®Œæˆ")
    
    return {
        "experiment_workorder": result.get("experiment_workorder"),
        "selected_optimization_name": result.get("selected_optimization_name"),
        "current_step": "workorder_generated",
        "workflow_status": "waiting_for_experiment"
    }


def await_experiment_results_node(state: CoatingWorkflowState) -> Dict:
    """ç­‰å¾…å®éªŒç»“æœèŠ‚ç‚¹ - ä½¿ç”¨Interruptæš‚åœå·¥ä½œæµ"""
    from langgraph.types import interrupt
    
    logger.info(f"[ç­‰å¾…å®éªŒ] ä»»åŠ¡ {state['task_id']}, è¿­ä»£ {state['current_iteration']}")
    
    # æš‚åœå·¥ä½œæµï¼Œç­‰å¾…å¤–éƒ¨è¾“å…¥
    experiment_data = interrupt({
        "action": "await_experiment_results",
        "task_id": state["task_id"],
        "iteration": state["current_iteration"],
        "workorder_id": state.get("experiment_workorder", {}).get("id")
    })
    
    logger.info(f"[å®éªŒç»“æœæ¥æ”¶] ä»»åŠ¡ {state['task_id']}, æ•°æ®: {experiment_data}")
    
    # éªŒè¯å®éªŒæ•°æ®å®Œæ•´æ€§
    required_fields = ["hardness", "adhesion_strength", "oxidation_temperature"]
    for field in required_fields:
        if field not in experiment_data:
            raise ValueError(f"å®éªŒæ•°æ®ç¼ºå°‘å¿…å¡«å­—æ®µ: {field}")
    
    # è®°å½•åˆ°è¿­ä»£å†å²
    iteration_record = {
        "iteration": state["current_iteration"],
        "timestamp": datetime.now().isoformat(),
        "parameters": {
            "composition": state["coating_composition"],
            "process": state["process_params"],
            "structure": state["structure_design"]
        },
        "prediction": state.get("performance_prediction"),
        "experiment_result": experiment_data
    }
    
    iteration_history = state.get("iteration_history", [])
    iteration_history.append(iteration_record)
    
    return {
        "experimental_results": experiment_data,
        "iteration_history": iteration_history,
        "current_iteration": state["current_iteration"] + 1,
        "current_step": "experiment_received",
        "workflow_status": "analyzing_results"
    }


def convergence_check_node(state: CoatingWorkflowState) -> Dict:
    """æ”¶æ•›æ£€æŸ¥èŠ‚ç‚¹ - åˆ¤æ–­æ˜¯å¦ç»§ç»­è¿­ä»£"""
    logger.info(f"[æ”¶æ•›æ£€æŸ¥] ä»»åŠ¡ {state['task_id']}, è¿­ä»£ {state['current_iteration']}")
    
    from ..services.convergence_service import ConvergenceService
    
    convergence_service = ConvergenceService()
    result = convergence_service.check_convergence(state)
    
    return {
        "convergence_achieved": result["is_converged"],
        "convergence_reason": result["reason"],
        "current_step": "convergence_checked",
        "workflow_status": "converged" if result["is_converged"] else "continuing"
    }
```

### 1.2 å·¥ä½œæµå›¾é‡æ„

**æ–°çš„å·¥ä½œæµç»“æ„ï¼š**

```python
# src/graph/workflow.py

def create_coating_workflow_with_iteration(
    use_memory: bool = True,
    enable_streaming: bool = True
) -> StateGraph:
    """åˆ›å»ºæ”¯æŒè¿­ä»£çš„æ¶‚å±‚ä¼˜åŒ–å·¥ä½œæµ"""
    workflow = StateGraph(CoatingWorkflowState)
    
    # ========== æ·»åŠ æ‰€æœ‰èŠ‚ç‚¹ ==========
    # åŸºç¡€èŠ‚ç‚¹
    workflow.add_node("input_validation", input_validation_node)
    workflow.add_node("error_handler", error_handler_node)
    
    # æ€§èƒ½é¢„æµ‹æ¨¡å—
    workflow.add_node("topphi_simulation", topphi_simulation_node)
    workflow.add_node("ml_prediction", ml_model_prediction_node)
    workflow.add_node("historical_comparison", historical_comparison_node)
    workflow.add_node("integrated_analysis", integrated_analysis_node)
    
    # ä¼˜åŒ–å»ºè®®æ¨¡å—
    workflow.add_node("p1_composition_optimization", p1_composition_optimization_node)
    workflow.add_node("p2_structure_optimization", p2_structure_optimization_node)
    workflow.add_node("p3_process_optimization", p3_process_optimization_node)
    workflow.add_node("optimization_summary", optimization_summary_node)
    
    # ğŸ†• è¿­ä»£ç›¸å…³èŠ‚ç‚¹
    workflow.add_node("experiment_workorder", experiment_workorder_generation_node)
    workflow.add_node("await_experiment_results", await_experiment_results_node)
    workflow.add_node("convergence_check", convergence_check_node)
    
    # ========== è®¾ç½®å·¥ä½œæµè·¯å¾„ ==========
    # å…¥å£ç‚¹
    workflow.set_entry_point("input_validation")
    
    # éªŒè¯åˆ†æ”¯
    workflow.add_conditional_edges(
        "input_validation",
        should_continue_after_validation,
        {
            "topphi_simulation": "topphi_simulation",
            "error_handler": "error_handler"
        }
    )
    
    # æ€§èƒ½é¢„æµ‹é“¾
    workflow.add_edge("topphi_simulation", "ml_prediction")
    workflow.add_edge("ml_prediction", "historical_comparison")
    workflow.add_edge("historical_comparison", "integrated_analysis")
    
    # ä¼˜åŒ–å»ºè®®é“¾
    workflow.add_edge("integrated_analysis", "p1_composition_optimization")
    workflow.add_edge("integrated_analysis", "p2_structure_optimization")
    workflow.add_edge("integrated_analysis", "p3_process_optimization")
    
    workflow.add_edge("p1_composition_optimization", "optimization_summary")
    workflow.add_edge("p2_structure_optimization", "optimization_summary")
    workflow.add_edge("p3_process_optimization", "optimization_summary")
    
    # ğŸ†• ä¼˜åŒ–æ±‡æ€»åç­‰å¾…ç”¨æˆ·é€‰æ‹©ï¼ˆé€šè¿‡Interruptï¼‰
    workflow.add_edge("optimization_summary", "experiment_workorder")
    
    # ğŸ†• å·¥å•ç”Ÿæˆåç­‰å¾…å®éªŒç»“æœ
    workflow.add_edge("experiment_workorder", "await_experiment_results")
    
    # ğŸ†• å®éªŒç»“æœæ¥æ”¶åæ£€æŸ¥æ”¶æ•›
    workflow.add_edge("await_experiment_results", "convergence_check")
    
    # ğŸ†• æ”¶æ•›æ£€æŸ¥çš„æ¡ä»¶åˆ†æ”¯
    workflow.add_conditional_edges(
        "convergence_check",
        should_continue_iteration,
        {
            "continue": "historical_comparison",  # å¾ªç¯å›å»
            "end": END  # æ”¶æ•›ï¼Œç»“æŸ
        }
    )
    
    # é”™è¯¯å¤„ç†ç»ˆç«¯
    workflow.add_edge("error_handler", END)
    
    # ========== ç¼–è¯‘å·¥ä½œæµ ==========
    # ğŸ†• ä½¿ç”¨SQLiteæŒä¹…åŒ–
    from langgraph.checkpoint.sqlite import SqliteSaver
    checkpointer = SqliteSaver.from_conn_string("workflow_checkpoints.db")
    
    if use_memory:
        memory = InMemoryStore()
        compiled = workflow.compile(checkpointer=checkpointer, store=memory)
    else:
        compiled = workflow.compile(checkpointer=checkpointer)
    
    logger.info("è¿­ä»£ä¼˜åŒ–å·¥ä½œæµåˆ›å»ºå®Œæˆ")
    return compiled


def should_continue_iteration(state: CoatingWorkflowState) -> Literal["continue", "end"]:
    """åˆ¤æ–­æ˜¯å¦ç»§ç»­è¿­ä»£"""
    # æ£€æŸ¥æ˜¯å¦æ”¶æ•›
    if state.get("convergence_achieved", False):
        logger.info(f"[è¿­ä»£ç»ˆæ­¢] ä»»åŠ¡ {state['task_id']}: {state.get('convergence_reason')}")
        return "end"
    
    # æ£€æŸ¥æœ€å¤§è¿­ä»£æ¬¡æ•°
    if state.get("current_iteration", 0) >= state.get("max_iterations", 5):
        logger.info(f"[è¿­ä»£ç»ˆæ­¢] ä»»åŠ¡ {state['task_id']}: è¾¾åˆ°æœ€å¤§è¿­ä»£æ¬¡æ•°")
        return "end"
    
    logger.info(f"[ç»§ç»­è¿­ä»£] ä»»åŠ¡ {state['task_id']}, ç¬¬ {state['current_iteration']} è½®")
    return "continue"
```

### 1.3 æ–°å¢æœåŠ¡

**âœ… å·¥å•ç”ŸæˆæœåŠ¡å·²å­˜åœ¨**

ç°æœ‰çš„ `src/services/workorder_service.py` å·²ç»æä¾›äº†å®Œå–„çš„å·¥å•ç”ŸæˆåŠŸèƒ½ï¼š
- `generate_workorder(task_id, selected_option, task_state, stream_callback)` å‡½æ•°
- æ”¯æŒP1/P2/P3ä¸‰ç§æ–¹æ¡ˆ
- ä½¿ç”¨LLMç”Ÿæˆä¸“ä¸šçš„å®éªŒå·¥å•
- æ”¯æŒæµå¼è¾“å‡º

**âš ï¸ æ— éœ€åˆ›å»ºæ–°çš„WorkorderServiceç±»ï¼** ç›´æ¥ä½¿ç”¨ç°æœ‰å‡½æ•°å³å¯ã€‚

---

**ConvergenceServiceï¼ˆæ”¶æ•›åˆ¤æ–­æœåŠ¡ï¼‰ï¼š**

âœ¨ **éœ€è¦æ–°å»º**

```python
# src/services/convergence_service.py

class ConvergenceService:
    """æ”¶æ•›åˆ¤æ–­æœåŠ¡"""
    
    def check_convergence(self, state: CoatingWorkflowState) -> Dict:
        """æ£€æŸ¥æ˜¯å¦è¾¾åˆ°æ”¶æ•›æ¡ä»¶"""
        target = state.get("target_requirements", {})
        experiment = state.get("experimental_results", {})
        history = state.get("iteration_history", [])
        
        # æ¡ä»¶1ï¼šæ€§èƒ½è¾¾æ ‡
        performance_met = self._check_performance_target(experiment, target)
        
        # æ¡ä»¶2ï¼šè¿ç»­æ”¹å–„ç‡ä½
        improvement_stalled = self._check_improvement_trend(history)
        
        # æ¡ä»¶3ï¼šé¢„æµ‹ä¸å®é™…è¯¯å·®å°
        prediction_accurate = self._check_prediction_accuracy(state)
        
        if performance_met:
            return {
                "is_converged": True,
                "reason": "æ€§èƒ½æŒ‡æ ‡è¾¾æ ‡",
                "details": {
                    "hardness_achieved": experiment.get("hardness"),
                    "target_hardness": target.get("min_hardness")
                }
            }
        
        if improvement_stalled and len(history) >= 3:
            return {
                "is_converged": True,
                "reason": "è¿ç»­3è½®æ”¹å–„ä¸æ˜æ˜¾ï¼Œå»ºè®®ç»ˆæ­¢",
                "details": {"recent_improvements": [...]}
            }
        
        return {
            "is_converged": False,
            "reason": "ç»§ç»­ä¼˜åŒ–",
            "current_gap": {
                "hardness": target.get("min_hardness", 0) - experiment.get("hardness", 0),
                ...
            }
        }
    
    def _check_performance_target(self, experiment: Dict, target: Dict) -> bool:
        """æ£€æŸ¥æ€§èƒ½æ˜¯å¦è¾¾æ ‡"""
        if not experiment or not target:
            return False
        
        checks = [
            experiment.get("hardness", 0) >= target.get("min_hardness", 999),
            experiment.get("adhesion_strength", 0) >= target.get("min_adhesion", 999),
            experiment.get("oxidation_temperature", 0) >= target.get("min_oxidation", 999)
        ]
        
        return all(checks)
    
    def _check_improvement_trend(self, history: List[Dict]) -> bool:
        """æ£€æŸ¥æ”¹å–„è¶‹åŠ¿æ˜¯å¦åœæ»"""
        if len(history) < 3:
            return False
        
        recent_3 = history[-3:]
        improvements = []
        
        for i in range(1, len(recent_3)):
            prev_hardness = recent_3[i-1]["experiment_result"].get("hardness", 0)
            curr_hardness = recent_3[i]["experiment_result"].get("hardness", 0)
            improvement = (curr_hardness - prev_hardness) / max(prev_hardness, 1)
            improvements.append(improvement)
        
        # è¿ç»­æ”¹å–„ç‡éƒ½å°äº5%
        return all(imp < 0.05 for imp in improvements)
    
    def _check_prediction_accuracy(self, state: CoatingWorkflowState) -> bool:
        """æ£€æŸ¥é¢„æµ‹å‡†ç¡®æ€§"""
        prediction = state.get("performance_prediction", {})
        experiment = state.get("experimental_results", {})
        
        if not prediction or not experiment:
            return False
        
        pred_hardness = prediction.get("hardness", 0)
        exp_hardness = experiment.get("hardness", 0)
        
        error_rate = abs(pred_hardness - exp_hardness) / max(exp_hardness, 1)
        
        return error_rate < 0.10  # è¯¯å·®å°äº10%
```

---

## 2ï¸âƒ£ WebSocket APIæ‰©å±•

### 2.1 æ–°å¢æ¶ˆæ¯ç±»å‹

```python
# src/api/routes/websocket_routes.py

async def handle_websocket_message(message: Dict, client_id: str):
    """å¤„ç†WebSocketæ¶ˆæ¯"""
    message_type = message.get("type")
    data = message.get("data", {})
    current_task_id = message.get("task_id")
    
    if message_type == "start_workflow":
        await handle_start_workflow(data, client_id)
    
    # ğŸ†• æäº¤å®éªŒç»“æœ
    elif message_type == "submit_experiment_results":
        await handle_submit_experiment_results(data, client_id, current_task_id)
    
    # ğŸ†• é€‰æ‹©ä¼˜åŒ–æ–¹æ¡ˆï¼ˆç§»å…¥å·¥ä½œæµå†…éƒ¨å¤„ç†ï¼‰
    elif message_type == "select_optimization":
        await handle_select_optimization(data, client_id, current_task_id)
    
    elif message_type == "get_state":
        await handle_get_state(client_id, current_task_id)
    
    elif message_type == "reconnect":
        await handle_reconnect(data, client_id)
    
    else:
        await manager.send_json({
            "type": "error",
            "message": f"ä¸æ”¯æŒçš„æ¶ˆæ¯ç±»å‹: {message_type}"
        }, client_id)


async def handle_submit_experiment_results(
    data: Dict, 
    client_id: str, 
    task_id: str
):
    """å¤„ç†å®éªŒç»“æœæäº¤"""
    if not task_id:
        task_id = manager.get_task_id(client_id)
    
    if not task_id:
        await manager.send_json({
            "type": "error",
            "message": "æ²¡æœ‰æ´»åŠ¨çš„ä»»åŠ¡"
        }, client_id)
        return
    
    try:
        # éªŒè¯å®éªŒæ•°æ®
        experiment_data = data.get("experiment_results")
        if not experiment_data:
            raise ValueError("ç¼ºå°‘å®éªŒæ•°æ®")
        
        required_fields = ["hardness", "adhesion_strength", "oxidation_temperature"]
        for field in required_fields:
            if field not in experiment_data:
                raise ValueError(f"ç¼ºå°‘å¿…å¡«å­—æ®µ: {field}")
        
        logger.info(f"[å®éªŒç»“æœæäº¤] ä»»åŠ¡ {task_id}: {experiment_data}")
        
        # ğŸ”‘ ä½¿ç”¨Commandæ¢å¤å·¥ä½œæµï¼Œä¼ å…¥å®éªŒæ•°æ®
        from langgraph.types import Command
        
        resume_command = Command(
            resume=experiment_data,  # ä¼ é€’ç»™interruptçš„è¿”å›å€¼
            update={"experimental_results": experiment_data}  # æ›´æ–°state
        )
        
        # æ¢å¤å·¥ä½œæµæ‰§è¡Œ
        asyncio.create_task(
            execute_workflow_stream(
                task_id, 
                None,  # thread_idä»checkpointerä¸­æ¢å¤
                resume_command,  # ä½¿ç”¨Commandæ¢å¤
                client_id
            )
        )
        
        await manager.send_json({
            "type": "experiment_received",
            "message": "å®éªŒç»“æœå·²æäº¤ï¼Œç»§ç»­åˆ†æ..."
        }, client_id)
        
    except Exception as e:
        logger.error(f"[å®éªŒç»“æœæäº¤å¤±è´¥] {str(e)}")
        await manager.send_json({
            "type": "error",
            "message": f"æäº¤å¤±è´¥: {str(e)}"
        }, client_id)


async def handle_select_optimization(
    data: Dict,
    client_id: str,
    task_id: str
):
    """å¤„ç†ä¼˜åŒ–æ–¹æ¡ˆé€‰æ‹©"""
    if not task_id:
        task_id = manager.get_task_id(client_id)
    
    selected_type = data.get("selected_type")  # "P1", "P2", "P3"
    selected_plan = data.get("selected_plan")  # å…·ä½“æ–¹æ¡ˆæ•°æ®
    
    logger.info(f"[æ–¹æ¡ˆé€‰æ‹©] ä»»åŠ¡ {task_id}: {selected_type}")
    
    # æ›´æ–°stateä¸­çš„é€‰æ‹©
    workflow_manager.update_task_selection(task_id, {
        "type": selected_type,
        "plan": selected_plan
    })
    
    # ğŸ”‘ ç»§ç»­å·¥ä½œæµï¼ˆä»optimization_summary â†’ experiment_workorderï¼‰
    # å·¥ä½œæµä¼šè‡ªåŠ¨æ‰§è¡Œï¼Œæ— éœ€Command
    
    await manager.send_json({
        "type": "selection_confirmed",
        "message": f"å·²é€‰æ‹©{selected_type}æ–¹æ¡ˆï¼Œæ­£åœ¨ç”Ÿæˆå·¥å•..."
    }, client_id)
```

### 2.2 æ‰§è¡Œå‡½æ•°ä¿®æ”¹

```python
async def execute_workflow_stream(
    task_id: str, 
    thread_id: str, 
    input_data: Any,  # å¯ä»¥æ˜¯Dictæˆ–Command
    client_id: str
):
    """æµå¼æ‰§è¡Œå·¥ä½œæµ"""
    try:
        # æ£€æŸ¥æ˜¯å¦æ˜¯æ¢å¤æ‰§è¡Œï¼ˆCommandå¯¹è±¡ï¼‰
        from langgraph.types import Command
        is_resume = isinstance(input_data, Command)
        
        if is_resume:
            logger.info(f"[å·¥ä½œæµæ¢å¤] ä»»åŠ¡ {task_id}")
        else:
            logger.info(f"[å·¥ä½œæµå¯åŠ¨] ä»»åŠ¡ {task_id}")
        
        # æµå¼æ‰§è¡Œ
        async for event_type, event_data in workflow_manager.stream_task(
            task_id, input_data, thread_id
        ):
            cleaned_data = clean_data_for_json(event_data)
            
            # è¯¦ç»†æ—¥å¿—
            logger.info(f"[WSå‘é€] type={event_type}, dataé”®={list(cleaned_data.keys())}")
            
            # å‘é€åˆ°å‰ç«¯
            await manager.send_json({
                "type": event_type,
                "data": cleaned_data
            }, client_id)
            
            # çŸ­æš‚å»¶è¿Ÿï¼Œé¿å…æ¶ˆæ¯è¿‡å¿«
            await asyncio.sleep(0.01)
        
        # å·¥ä½œæµå®Œæˆæˆ–æš‚åœ
        logger.info(f"[å·¥ä½œæµå®Œæˆ/æš‚åœ] ä»»åŠ¡ {task_id}")
        
        await manager.send_json({
            "type": "workflow_paused" if is_resume else "workflow_completed",
            "task_id": task_id
        }, client_id)
        
    except Exception as e:
        logger.error(f"[å·¥ä½œæµæ‰§è¡Œå¤±è´¥] {str(e)}", exc_info=True)
        await manager.send_json({
            "type": "error",
            "message": f"æ‰§è¡Œå¤±è´¥: {str(e)}"
        }, client_id)
```

---

## ğŸ“Œ ä¸‹ä¸€æ­¥ï¼šæŸ¥çœ‹ç¬¬3éƒ¨åˆ†

å‰ç«¯UIè®¾è®¡å’Œå®Œæ•´å®æ–½è®¡åˆ’è¯·æŸ¥çœ‹ï¼š
- `ITERATION_PLAN_PART3_IMPLEMENTATION.md`
